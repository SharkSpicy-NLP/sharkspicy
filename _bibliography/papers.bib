---
---

@string{aps = {American Physical Society,}}

@misc{yu2025srkiscalablerealtimeknowledge,
    abbr = {AAAI},
    title={SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention}, 
    author={Bohan Yu and Wei Huang and Kang Liu},
    year={2025},
    eprint={2511.06446},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2511.06446}, 
    abstract = {This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.},
    pdf = {https://arxiv.org/pdf/2511.06446},
    code = {https://github.com/SharkSpicy-NLP/SR-KI}
}

@inproceedings{yu-chai-2025-evolkv,
    abbr = {EMNLP-Findings},
    title = "{E}vol{KV}: Evolutionary {KV} Cache Compression for {LLM} Inference",
    author = "Bohan Yu  and
      Yekun Chai",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2025",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-emnlp.88/",
    doi = "10.18653/v1/2025.findings-emnlp.88",
    pages = "1673--1689",
    ISBN = "979-8-89176-335-7",
    abstract = "Existing key-value (KV) cache compression methods typically rely on heuristics, such as uniform cache allocation across layers or static eviction policies, however, they ignore the critical interplays among layer-specific feature patterns and task performance, which can lead to degraded generalization. In this paper, we propose EvolKV, an adaptive framework for layer-wise, task-driven KV cache compression that jointly optimizes the memory efficiency and task performance. By reformulating cache allocation as a multi-objective optimization problem, EvolKV leverages evolutionary search to dynamically configure layer budgets while directly maximizing downstream performance. Extensive experiments on 11 tasks demonstrate that our approach outperforms all baseline methods across a wide range of KV cache budgets on long-context tasks and surpasses heuristic baselines by up to 7 percentage points on GSM8K. Notably, EvolKV achieves superior performance over the full KV cache setting on code completion while utilizing only 1.5{\%} of the original budget, suggesting the untapped potential in learned compression strategies for KV cache budget allocation.",
    pdf = {https://aclanthology.org/2025.findings-emnlp.88.pdf}
}

@inproceedings{zhu-etal-2025-tableeval,
    abbr = {EMNLP},
    title = "{T}able{E}val: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering",
    author = "Junnan Zhu  and
      Jingyi Wang  and
      Bohan Yu  and
      Xiaoyu Wu  and
      Junbo Li  and
      Lei Wang  and
      Nan Xu",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.363/",
    doi = "10.18653/v1/2025.emnlp-main.363",
    pages = "7126--7146",
    ISBN = "979-8-89176-332-6",
    abstract = "LLMs have shown impressive progress in natural language processing. However, they still face significant challenges in TableQA, where real-world complexities such as diverse table structures, multilingual data, and domain-specific reasoning are crucial. Existing TableQA benchmarks are often limited by their focus on simple flat tables and suffer from data leakage. Furthermore, most benchmarks are monolingual and fail to capture the cross-lingual and cross-domain variability in practical applications. To address these limitations, we introduce TableEval, a new benchmark designed to evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes tables with various structures (such as concise, hierarchical, and nested tables) collected from four domains (including government, finance, academia, and industry reports). Besides, TableEval features cross-lingual scenarios with tables in Simplified Chinese, Traditional Chinese, and English. To minimize the risk of data leakage, we collect all data from recent real-world documents. Considering that existing TableQA metrics fail to capture semantic accuracy, we further propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level. Experimental results have shown that SEAT achieves high agreement with human judgment. Extensive experiments on TableEval reveal critical gaps in the ability of state-of-the-art LLMs to handle these complex, real-world TableQA tasks, offering insights for future improvements.",
    pdf = {https://aclanthology.org/2025.emnlp-main.363.pdf},
    code = {https://github.com/wenge-research/TableEval},
    collaborative = {true}
}


@misc{han2026conformitydynamicsllmmultiagent,
      abbr = {Preprint},
      title={Conformity Dynamics in LLM Multi-Agent Systems: The Roles of Topology and Self-Social Weighting}, 
      author={Chen Han and Jin Tan and Bohan Yu and Wenzhen Zheng and Xijin Tang},
      year={2026},
      eprint={2601.05606},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2601.05606}, 
      abstract={Large Language Models (LLMs) are increasingly instantiated as interacting agents in multi-agent systems (MAS), where collective decisions emerge through social interaction rather than independent reasoning. A fundamental yet underexplored mechanism in this process is conformity, the tendency of agents to align their judgments with prevailing group opinions. This paper presents a systematic study of how network topology shapes conformity dynamics in LLM-based MAS through a misinformation detection task. We introduce a confidence-normalized pooling rule that controls the trade-off between self-reliance and social influence, enabling comparisons between two canonical decision paradigms: Centralized Aggregation and Distributed Consensus. Experimental results demonstrate that network topology critically governs both the efficiency and robustness of collective judgments. Centralized structures enable immediate decisions but are sensitive to hub competence and exhibit same-model alignment biases. In contrast, distributed structures promote more robust consensus, while increased network connectivity speeds up convergence but also heightens the risk of wrong-but-sure cascades, in which agents converge on incorrect decisions with high confidence. These findings characterize the conformity dynamics in LLM-based MAS, clarifying how network topology and self-social weighting jointly shape the efficiency, robustness, and failure modes of collective decision-making.},
      collaborative = {true}
}

@misc{zhou2026taskstratifiedknowledgescalinglaws,
      abbr = {Preprint},
      title={Task-Stratified Knowledge Scaling Laws for Post-Training Quantized Large Language Models}, 
      author={Chenxi Zhou and Pengfei Cao and Jiang Li and Bohan Yu and Jinyu Ye and Jun Zhao and Kang Liu},
      year={2026},
      eprint={2508.18609},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.18609}, 
      abstract={Post-Training Quantization (PTQ) is a critical strategy for efficient Large Language Models (LLMs) deployment. However, existing scaling laws primarily focus on general performance, overlooking crucial fine-grained factors and how quantization differentially impacts diverse knowledge capabilities. To address this, we establish Task-Stratified Knowledge Scaling Laws. By stratifying capabilities into memorization, application, and reasoning, we develop a framework that unifies model size, bit-width, and fine-grained factors: group size and calibration set size. Validated on 293 diverse PTQ configurations, our framework demonstrates strong fit and cross-architecture consistency. It reveals distinct sensitivities across knowledge capabilities: reasoning is precision-critical, application is scale-responsive, and memorization is calibration-sensitive. We highlight that in low-bit scenarios, optimizing these fine-grained factors is essential for preventing performance collapse. These findings provide an empirically-backed foundation for designing knowledge-aware quantization strategies.},
      collaborative = {true}
}