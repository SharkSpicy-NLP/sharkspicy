---
---

@string{aps = {American Physical Society,}}

@misc{yu2025srkiscalablerealtimeknowledge,
    abbr = {AAAI},
    title={SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention}, 
    author={Bohan Yu and Wei Huang and Kang Liu},
    year={2025},
    eprint={2511.06446},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2511.06446}, 
    selected = {true}
}

% @inproceedings{yu-chai-2025-evolkv,
%     abbr = {EMNLP-Findings},
%     title = "{E}vol{KV}: Evolutionary {KV} Cache Compression for {LLM} Inference",
%     author = "Bohan Yu  and
%       Yekun Chai",
%     booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2025",
%     month = nov,
%     year = "2025",
%     address = "Suzhou, China",
%     publisher = "Association for Computational Linguistics",
%     url = "https://aclanthology.org/2025.findings-emnlp.88/",
%     doi = "10.18653/v1/2025.findings-emnlp.88",
%     pages = "1673--1689",
%     ISBN = "979-8-89176-335-7",
%     abstract = "Existing key-value (KV) cache compression methods typically rely on heuristics, such as uniform cache allocation across layers or static eviction policies, however, they ignore the critical interplays among layer-specific feature patterns and task performance, which can lead to degraded generalization. In this paper, we propose EvolKV, an adaptive framework for layer-wise, task-driven KV cache compression that jointly optimizes the memory efficiency and task performance. By reformulating cache allocation as a multi-objective optimization problem, EvolKV leverages evolutionary search to dynamically configure layer budgets while directly maximizing downstream performance. Extensive experiments on 11 tasks demonstrate that our approach outperforms all baseline methods across a wide range of KV cache budgets on long-context tasks and surpasses heuristic baselines by up to 7 percentage points on GSM8K. Notably, EvolKV achieves superior performance over the full KV cache setting on code completion while utilizing only 1.5{\%} of the original budget, suggesting the untapped potential in learned compression strategies for KV cache budget allocation.",
%     selected = {true}
% }

% @inproceedings{zhu-etal-2025-tableeval,
%     abbr = {EMNLP},
%     title = "{T}able{E}val: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering",
%     author = "Junnan Zhu  and
%       Jingyi Wang  and
%       Bohan Yu  and
%       Xiaoyu Wu  and
%       Junbo Li  and
%       Lei Wang  and
%       Nan Xu",
%     booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
%     month = nov,
%     year = "2025",
%     address = "Suzhou, China",
%     publisher = "Association for Computational Linguistics",
%     url = "https://aclanthology.org/2025.emnlp-main.363/",
%     doi = "10.18653/v1/2025.emnlp-main.363",
%     pages = "7126--7146",
%     ISBN = "979-8-89176-332-6",
%     abstract = "LLMs have shown impressive progress in natural language processing. However, they still face significant challenges in TableQA, where real-world complexities such as diverse table structures, multilingual data, and domain-specific reasoning are crucial. Existing TableQA benchmarks are often limited by their focus on simple flat tables and suffer from data leakage. Furthermore, most benchmarks are monolingual and fail to capture the cross-lingual and cross-domain variability in practical applications. To address these limitations, we introduce TableEval, a new benchmark designed to evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes tables with various structures (such as concise, hierarchical, and nested tables) collected from four domains (including government, finance, academia, and industry reports). Besides, TableEval features cross-lingual scenarios with tables in Simplified Chinese, Traditional Chinese, and English. To minimize the risk of data leakage, we collect all data from recent real-world documents. Considering that existing TableQA metrics fail to capture semantic accuracy, we further propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level. Experimental results have shown that SEAT achieves high agreement with human judgment. Extensive experiments on TableEval reveal critical gaps in the ability of state-of-the-art LLMs to handle these complex, real-world TableQA tasks, offering insights for future improvements.",
%     selected = {true}
% }

% @article{PhysRev.47.777,
%   abbr              = {PhysRev},
%   title             = {Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
%   author            = {Einstein*†, A. and Podolsky*, B. and Rosen*, N.},
%   abstract          = {In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
%   journal           = {Phys. Rev.},
%   location          = {New Jersey},
%   volume            = {47},
%   issue             = {10},
%   pages             = {777--780},
%   numpages          = {0},
%   year              = {1935},
%   month             = {May},
%   publisher         = aps,
%   doi               = {10.1103/PhysRev.47.777},
%   url               = {https://link.aps.org/doi/10.1103/PhysRev.47.777},
%   html              = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
%   pdf               = {example_pdf.pdf},
%   altmetric         = {248277},
%   dimensions        = {true},
%   google_scholar_id = {qyhmnyLat1gC},
%   video             = {https://www.youtube-nocookie.com/embed/aqz-KE-bpKQ},
%   additional_info   = {. *More Information* can be [found here](https://github.com/alshedivat/al-folio/)},
%   annotation        = {* Example use of superscripts<br>† Albert Einstein},
%   selected          = {true},
%   inspirehep_id     = {3255}
% }


% @book{przibram1967letters,
%   bibtex_show = {true},
%   title       = {Letters on wave mechanics},
%   author      = {Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
%   year        = {1967},
%   publisher   = {Vision},
%   preview     = {wave-mechanics.gif},
%   abbr        = {Vision}
% }
